{
    "nbformat": 4,
    "nbformat_minor": 5,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "code",
            "source": [
                "!pip install unsloth"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 1000
                },
                "id": "LD3uwoRGtkjb",
                "outputId": "30bec759-9873-42cc-ee5d-3c801573e5c5"
            },
            "execution_count": null,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Collecting unsloth\n",
                        "  Downloading unsloth-2025.4.7-py3-none-any.whl.metadata (46 kB)\n",
                        "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/46.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hCollecting unsloth_zoo>=2025.4.4 (from unsloth)\n",
                        "  Downloading unsloth_zoo-2025.4.4-py3-none-any.whl.metadata (8.0 kB)\n",
                        "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.6.0+cu124)\n",
                        "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
                        "  Downloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
                        "Collecting bitsandbytes (from unsloth)\n",
                        "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
                        "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.2.0)\n",
                        "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth) (24.2)\n",
                        "Collecting tyro (from unsloth)\n",
                        "  Downloading tyro-0.9.19-py3-none-any.whl.metadata (9.9 kB)\n",
                        "Requirement already satisfied: transformers!=4.47.0,>=4.46.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.51.3)\n"
                    ]
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "application/vnd.colab-display-data+json": {
                            "pip_warning": {
                                "packages": [
                                    "google",
                                    "nvidia"
                                ]
                            },
                            "id": "057ed1b7287c4445a606bff9c7b9d70e"
                        }
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "import unsloth"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "3vAQ-cfgVayr",
                "outputId": "f6281fb7-50b1-4677-bbbc-34924df230e0"
            },
            "execution_count": null,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
                        "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
                    ]
                }
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "from unsloth import FastLanguageModel\n",
                "import torch\n",
                "\n",
                "max_seq_lenth = 2048\n",
                "dtype=None\n",
                "load_in_4bit = True\n",
                "\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
                "    max_seq_length = max_seq_lenth,\n",
                "    load_in_4bit = load_in_4bit,\n",
                "    dtype = dtype\n",
                ")"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "a9HfORp9YJ8N",
                "outputId": "c9cdee10-b2d9-4347-8691-877537deb253"
            },
            "execution_count": null,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "==((====))==  Unsloth 2025.4.7: Fast Llama patching. Transformers: 4.51.3.\n",
                        "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
                        "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
                        "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n",
                        " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
                        "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
                    ]
                }
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "model = FastLanguageModel.get_peft_model(\n",
                "    model,\n",
                "    r = 16,\n",
                "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "    lora_alpha = 16,\n",
                "    lora_dropout = 0,\n",
                "    bias = \"none\",\n",
                "    use_gradient_checkpointing = \"unsloth\",\n",
                "    random_state = 3407,\n",
                "    use_rslora = False,\n",
                "    loftq_config = None,\n",
                ")"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "Ba4ss2O9ZHlC",
                "outputId": "a5eccc9b-a1ed-4209-8c21-16a526dba514"
            },
            "execution_count": null,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "Unsloth 2025.4.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
                    ]
                }
            ]
        },
        {
            "cell_type": "code",
            "source": [],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 81,
                    "referenced_widgets": [
                        "dff0ab1c598a48dca1b7305951b1a6c8",
                        "619dab9a688c458f835eb14169aa67db",
                        "d6eabdb524cb4bb28f77421ab3cd2c78",
                        "0ef12462333b4927b3abed458a7f8ba0",
                        "0fdbce6c8a8c460b8c3170153605a931",
                        "26e4f21ea90b43ffaef8e5193f19e03f",
                        "05dbc05f52744bb0b16dffc77f63a7a1",
                        "0b31ffe81fe04fdd932409f47c6f1e9b",
                        "ef9646b670ac4277bb08e514de417d97",
                        "5f65e89829a7416e9ed2d72c22470e1d",
                        "d31b485ea62840838aaa9434fd033e9d",
                        "652f8ca917084a009e46e25dcc2363dc",
                        "721375bbc9bb4a2cb8a6cc432035e78a",
                        "42143519cc2e4933b9162c323700fde6",
                        "06736b6cda7f4ca290c7f93406f96454",
                        "4d6acc9337e54f75bda03b856f822a1e",
                        "232f1cbad88f4476b0d25d76aca81403",
                        "2b03eeb61c084220ac12d97455b35ee5",
                        "02283df1c5634ec4be77ba3f413c36f8",
                        "d4a6bca32b6d45c3b31cfdf487c261ee",
                        "81038c4e10c64a7184d3fca5e7234207",
                        "6e874119f8954c1581d6aeb34afd4d65"
                    ]
                },
                "id": "D0_UiLaIhp5T",
                "outputId": "94887868-42d4-4fcc-bc47-50fbeb9d1e5f"
            },
            "execution_count": null,
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "Generating train split:   0%|          | 0/100000 [00:00<?, ? examples/s]"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "dff0ab1c598a48dca1b7305951b1a6c8"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "Generating test split:   0%|          | 0/5851 [00:00<?, ? examples/s]"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "652f8ca917084a009e46e25dcc2363dc"
                        }
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "alpeca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
                "\n",
                "### Instruction:\n",
                "Company database: {}\n",
                "\n",
                "### Input:\n",
                "SQL Prompt: {}\n",
                "\n",
                "### Response:\n",
                "SQL:\n",
                "\n",
                "Explanation: {}\n",
                "\"\"\""
            ],
            "metadata": {
                "id": "zE7ikkcuf4do"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "EOS_TOKEN = tokenizer.eos_token\n",
                "\n",
                "def formatting_prompts(examples):\n",
                "  company_database = examples[\"sql_context\"]\n",
                "  prompts = examples[\"sql_prompt\"]\n",
                "  sqls = examples[\"sql\"]\n",
                "  explanations = examples[\"sql_explanation\"]\n",
                "  texts = []\n",
                "  for company_database, prompts, sqls, explanations in zip(company_database, prompts, sqls, explanations):\n",
                "    text = alpeca_prompt.format(company_database, prompts, sqls, explanations) + EOS_TOKEN\n",
                "    texts.append(text)\n",
                "  return {\"text\" : texts, }\n",
                "pass\n",
                "\n",
                "from datasets import load_dataset\n",
                "\n",
                "ds = load_dataset(\"gretelai/synthetic_text_to_sql\", split = \"train\")\n",
                "ds = ds.map(formatting_prompts, batched = True)"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 49,
                    "referenced_widgets": [
                        "b8fa8ac66ad449ed8d5d2456d01a9d30",
                        "4cab4a9a40ed4d3ab59a57792fa523f2",
                        "33dedb3eb2b94b39aca9a6c523f82eec",
                        "2a886e14dfb542e1b8855369dc14ac01",
                        "0544ae1b998d415394f485456c86e0d0",
                        "877a2df5c86b4fa3907fcac794313d4a",
                        "aefaacb64029461a93bc2ba4c39f5b64",
                        "4683dbdc3a104ee4a55522d126db73a4",
                        "94479fa1008f4a57801e0eaa486b277b",
                        "854a3b21a17b4e74b01c374fee9a2767",
                        "e53b69e5f33f4a6aac2a00ef24a4535f"
                    ]
                },
                "id": "Aro9wFSlh17S",
                "outputId": "0d7e48a6-bdaa-4905-a056-37f9e191e4ea"
            },
            "execution_count": null,
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "b8fa8ac66ad449ed8d5d2456d01a9d30"
                        }
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "ds[\"text\"]"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "hK07qKiomJSs",
                "outputId": "18188b4a-430d-4d0f-ea0c-c35f84ff9235"
            },
            "execution_count": null,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "[\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nCompany database: CREATE TABLE salesperson (salesperson_id INT, name TEXT, region TEXT); INSERT INTO salesperson (salesperson_id, name, region) VALUES (1, 'John Doe', 'North'), (2, 'Jane Smith', 'South'); CREATE TABLE timber_sales (sales_id INT, salesperson_id INT, volume REAL, sale_date DATE); INSERT INTO timber_sales (sales_id, salesperson_id, volume, sale_date) VALUES (1, 1, 120, '2021-01-01'), (2, 1, 150, '2021-02-01'), (3, 2, 180, '2021-01-01');\n\n### Input:\nSQL Prompt: What is the total volume of timber sold by each salesperson, sorted by salesperson?\n\n### Response:\nSQL: \n\nExplanation: SELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY total_volume DESC; \n<|end_of_text|>\",\n",
                            " 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nCompany database: CREATE TABLE equipment_maintenance (equipment_type VARCHAR(255), maintenance_frequency INT);\n\n### Input:\nSQL Prompt: List all the unique equipment types and their corresponding total maintenance frequency from the equipment_maintenance table.\n\n### Response:\nSQL: \n\nExplanation: SELECT equipment_type, SUM(maintenance_frequency) AS total_maintenance_frequency FROM equipment_maintenance GROUP BY equipment_type; \n<|end_of_text|>',\n",
                            " \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nCompany database: CREATE TABLE marine_species (name VARCHAR(50), common_name VARCHAR(50), location VARCHAR(50));\n\n### Input:\nSQL Prompt: How many marine species are found in the Southern Ocean?\n\n### Response:\nSQL: \n\nExplanation: SELECT COUNT(*) FROM marine_species WHERE location = 'Southern Ocean'; \n<|end_of_text|>\",\n",
                            " \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nCompany database: CREATE TABLE ClientInvestments (ClientID INT, InvestmentType VARCHAR(20), Value FLOAT); INSERT INTO ClientInvestments (ClientID, InvestmentType, Value) VALUES (1, 'Stock', 10000), (1, 'Bond', 20000), (2, 'Stock', 30000), (2, 'Bond', 15000), (3, 'Stock', 5000), (3, 'Bond', 25000), (4, 'Stock', 40000), (4, 'Bond', 30000), (5, 'Stock', 7000), (5, 'Bond', 18000); CREATE TABLE Clients (ClientID INT, State VARCHAR(20)); INSERT INTO Clients (ClientID, State) VALUES (1, 'NY'), (2, 'TX'), (3, 'CA'), (4, 'NY'), (5, 'TX');\n\n### Input:\nSQL Prompt: What is the total value of investments in bonds for clients residing in Texas?\n\n### Response:\nSQL: \n\nExplanation: SELECT SUM(Value) FROM ClientInvestments CI JOIN Clients C ON CI.ClientID = C.ClientID WHERE C.State = 'TX' AND InvestmentType = 'Bond'; \n<|end_of_text|>\",\n",
                            " ...]"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 12
                }
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "from trl import SFTTrainer\n",
                "from transformers import TrainingArguments\n",
                "from unsloth import is_bf16_supported\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model = model,\n",
                "    tokenizer = tokenizer,\n",
                "    train_dataset = ds,\n",
                "    dataset_text_field = \"text\",\n",
                "    max_seq_length = max_seq_lenth,\n",
                "    dataset_num_proc = 2,\n",
                "    packing = False,\n",
                "    args = TrainingArguments(\n",
                "        per_device_train_batch_size = 2,\n",
                "        gradient_accumulation_steps = 4,\n",
                "        warmup_steps = 5,\n",
                "        max_steps = 60,\n",
                "        learning_rate = 2e-4,\n",
                "        fp16 = not is_bf16_supported(),\n",
                "        bf16 = is_bf16_supported(),\n",
                "        logging_steps = 1,\n",
                "        optim = \"adamw_8bit\",\n",
                "        weight_decay = 0.01,\n",
                "        lr_scheduler_type = \"linear\",\n",
                "        seed = 3407,\n",
                "\n",
                "\n",
                "    )\n",
                ")"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 49,
                    "referenced_widgets": [
                        "218f607a2fd948c79f128fe7dbf26b59",
                        "871cc20d272c4fb1a63bc973bbb15e29",
                        "3c9c47285d9c4767b0210a76243b1f76",
                        "e94a6f254bc1487e8839598b3f5ae53f",
                        "89b35b48c33644c88fae12d2c64ef80d",
                        "b210f55a3f774aac936c1a266b7977a4",
                        "7e15470c2bff454fbd135949dd8a7472",
                        "b9f19f8add7b44ae9dde75ebba9d2bc1",
                        "b482d8b6fb3b4b93a1f8f2e0ec44eb25",
                        "91be8696528e4ced9d1b6ba0b763a36f",
                        "e12dfb860c6045619b98a0b0f629cead"
                    ]
                },
                "id": "_8u8NE77m1tI",
                "outputId": "cd8da5b8-1782-414e-9cf8-08064d8adb94"
            },
            "execution_count": null,
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/100000 [00:00<?, ? examples/s]"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "218f607a2fd948c79f128fe7dbf26b59"
                        }
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "trainer_stats = trainer.train()"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 1000
                },
                "id": "0WixXnUmoobl",
                "outputId": "88e7c1ca-f8f5-4e08-c32f-351ee4f6b15a"
            },
            "execution_count": null,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
                        "   \\\\   /|    Num examples = 100,000 | Num Epochs = 1 | Total steps = 60\n",
                        "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
                        "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
                        " \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
                    ]
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "<IPython.core.display.Javascript object>"
                        ],
                        "application/javascript": [
                            "\n",
                            "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
                            "            function loadScript(url) {\n",
                            "            return new Promise(function(resolve, reject) {\n",
                            "                let newScript = document.createElement(\"script\");\n",
                            "                newScript.onerror = reject;\n",
                            "                newScript.onload = resolve;\n",
                            "                document.body.appendChild(newScript);\n",
                            "                newScript.src = url;\n",
                            "            });\n",
                            "            }\n",
                            "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
                            "            const iframe = document.createElement('iframe')\n",
                            "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
                            "            document.body.appendChild(iframe)\n",
                            "            const handshake = new Postmate({\n",
                            "                container: iframe,\n",
                            "                url: 'https://wandb.ai/authorize'\n",
                            "            });\n",
                            "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
                            "            handshake.then(function(child) {\n",
                            "                child.on('authorize', data => {\n",
                            "                    clearTimeout(timeout)\n",
                            "                    resolve(data)\n",
                            "                });\n",
                            "            });\n",
                            "            })\n",
                            "        });\n",
                            "    "
                        ]
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
                        "wandb: Paste an API key from your profile and hit enter:"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        " Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mumerfarooq230\u001b[0m (\u001b[33mumerfarooq230-riphah-international-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
                    ]
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ],
                        "text/html": [
                            "Tracking run with wandb version 0.19.10"
                        ]
                    },
                    "metadata": {}
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ],
                        "text/html": [
                            "Run data is saved locally in <code>/content/wandb/run-20250508_112822-01a0pl9c</code>"
                        ]
                    },
                    "metadata": {}
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ],
                        "text/html": [
                            "Syncing run <strong><a href='https://wandb.ai/umerfarooq230-riphah-international-university/huggingface/runs/01a0pl9c' target=\"_blank\">trainer_output</a></strong> to <a href='https://wandb.ai/umerfarooq230-riphah-international-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
                        ]
                    },
                    "metadata": {}
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ],
                        "text/html": [
                            " View project at <a href='https://wandb.ai/umerfarooq230-riphah-international-university/huggingface' target=\"_blank\">https://wandb.ai/umerfarooq230-riphah-international-university/huggingface</a>"
                        ]
                    },
                    "metadata": {}
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ],
                        "text/html": [
                            " View run at <a href='https://wandb.ai/umerfarooq230-riphah-international-university/huggingface/runs/01a0pl9c' target=\"_blank\">https://wandb.ai/umerfarooq230-riphah-international-university/huggingface/runs/01a0pl9c</a>"
                        ]
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Unsloth: Will smartly offload gradients to save VRAM!\n"
                    ]
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ],
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [60/60 06:19, Epoch 0/1]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Step</th>\n",
                            "      <th>Training Loss</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>1</td>\n",
                            "      <td>1.841900</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>2</td>\n",
                            "      <td>1.869100</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>3</td>\n",
                            "      <td>1.770600</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>4</td>\n",
                            "      <td>1.651000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>5</td>\n",
                            "      <td>1.489000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>6</td>\n",
                            "      <td>1.222900</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>7</td>\n",
                            "      <td>1.126300</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>8</td>\n",
                            "      <td>0.894100</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>9</td>\n",
                            "      <td>0.809100</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>10</td>\n",
                            "      <td>0.820400</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>11</td>\n",
                            "      <td>0.631100</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>12</td>\n",
                            "      <td>0.650700</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>13</td>\n",
                            "      <td>0.615000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>14</td>\n",
                            "      <td>0.560100</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>15</td>\n",
                            "      <td>0.553300</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>16</td>\n",
                            "      <td>0.568400</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>17</td>\n",
                            "      <td>0.621600</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>18</td>\n",
                            "      <td>0.542100</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>19</td>\n",
                            "      <td>0.605100</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>20</td>\n",
                            "      <td>0.581300</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>21</td>\n",
                            "      <td>0.505100</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>22</td>\n",
                            "      <td>0.483900</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>23</td>\n",
                            "      <td>0.490800</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>24</td>\n",
                            "      <td>0.506300</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>25</td>\n",
                            "      <td>0.446900</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>26</td>\n",
                            "      <td>0.502900</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>27</td>\n",
                            "      <td>0.506100</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>28</td>\n",
                            "      <td>0.560900</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>29</td>\n",
                            "      <td>0.467700</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>30</td>\n",
                            "      <td>0.440200</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>31</td>\n",
                            "      <td>0.531700</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>32</td>\n",
                            "      <td>0.491900</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>33</td>\n",
                            "      <td>0.464200</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>34</td>\n",
                            "      <td>0.515500</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>35</td>\n",
                            "      <td>0.502100</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>36</td>\n",
                            "      <td>0.433800</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>37</td>\n",
                            "      <td>0.479900</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>38</td>\n",
                            "      <td>0.435400</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>39</td>\n",
                            "      <td>0.427500</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>40</td>\n",
                            "      <td>0.434500</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>41</td>\n",
                            "      <td>0.474800</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>42</td>\n",
                            "      <td>0.483000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>43</td>\n",
                            "      <td>0.485600</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>44</td>\n",
                            "      <td>0.412600</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>45</td>\n",
                            "      <td>0.491100</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>46</td>\n",
                            "      <td>0.467200</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>47</td>\n",
                            "      <td>0.455600</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>48</td>\n",
                            "      <td>0.476800</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>49</td>\n",
                            "      <td>0.464800</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>50</td>\n",
                            "      <td>0.498100</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>51</td>\n",
                            "      <td>0.414700</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>52</td>\n",
                            "      <td>0.429200</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>53</td>\n",
                            "      <td>0.534800</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>54</td>\n",
                            "      <td>0.543100</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>55</td>\n",
                            "      <td>0.568700</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>56</td>\n",
                            "      <td>0.487300</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>57</td>\n",
                            "      <td>0.472900</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>58</td>\n",
                            "      <td>0.413000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>59</td>\n",
                            "      <td>0.454100</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>60</td>\n",
                            "      <td>0.461100</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ]
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "umzdHZXrox_J",
                "outputId": "6c261e33-b368-4acc-d31f-a61fc816cd6e"
            },
            "execution_count": null,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n",
                        "We shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\n",
                        "To force `safe_serialization`, set it to `None` instead.\n",
                        "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
                        "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
                        "Unsloth: Will remove a cached repo with size 6.0G\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
                        "Unsloth: Will use up to 4.64 out of 12.67 RAM for saving.\n",
                        "Unsloth: Saving model... This might take 5 minutes ...\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:01<00:01, 11.17it/s]\n",
                        "We will save to Disk and not RAM now.\n",
                        "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:48<00:00,  3.40s/it]\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Unsloth: Saving tokenizer... Done.\n",
                        "Unsloth: Saving model/pytorch_model-00001-of-00004.bin...\n",
                        "Unsloth: Saving model/pytorch_model-00002-of-00004.bin...\n",
                        "Unsloth: Saving model/pytorch_model-00003-of-00004.bin...\n",
                        "Unsloth: Saving model/pytorch_model-00004-of-00004.bin...\n",
                        "Done.\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "Unsloth: Converting llama model. Can use fast conversion = False.\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
                        "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
                        "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
                        "\\        /    [2] Converting GGUF 16bits to ['f16'] might take 10 minutes each.\n",
                        " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
                        "\n",
                        "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
                        "Unsloth: CMAKE detected. Finalizing some steps for installation.\n",
                        "Unsloth: [1] Converting model at model into f16 GGUF format.\n",
                        "The output location will be /content/model/unsloth.F16.gguf\n",
                        "This might take 3 minutes...\n",
                        "INFO:hf-to-gguf:Loading model: model\n",
                        "INFO:hf-to-gguf:Model architecture: LlamaForCausalLM\n",
                        "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
                        "INFO:hf-to-gguf:Exporting model...\n",
                        "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {64}\n",
                        "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
                        "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00004.bin'\n",
                        "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {4096, 128256}\n",
                        "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00002-of-00004.bin'\n",
                        "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00003-of-00004.bin'\n",
                        "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
                        "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
                        "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
                        "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00004-of-00004.bin'\n",
                        "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
                        "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {4096}\n",
                        "INFO:hf-to-gguf:output.weight,               torch.float16 --> F16, shape = {4096, 128256}\n",
                        "INFO:hf-to-gguf:Set meta model\n",
                        "INFO:hf-to-gguf:Set model parameters\n",
                        "INFO:hf-to-gguf:gguf: context length = 131072\n",
                        "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
                        "INFO:hf-to-gguf:gguf: feed forward length = 14336\n",
                        "INFO:hf-to-gguf:gguf: head count = 32\n",
                        "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
                        "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
                        "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
                        "INFO:hf-to-gguf:gguf: file type = 1\n",
                        "INFO:hf-to-gguf:Set model quantization version\n",
                        "INFO:hf-to-gguf:Set model tokenizer\n",
                        "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
                        "INFO:gguf.vocab:Adding 280147 merge(s).\n",
                        "INFO:gguf.vocab:Setting special token type bos to 128000\n",
                        "INFO:gguf.vocab:Setting special token type eos to 128001\n",
                        "INFO:gguf.vocab:Setting special token type pad to 128004\n",
                        "INFO:gguf.vocab:Setting add_bos_token to True\n",
                        "INFO:gguf.gguf_writer:Writing the following files:\n",
                        "INFO:gguf.gguf_writer:/content/model/unsloth.F16.gguf: n_tensors = 292, total_size = 16.1G\n",
                        "Writing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16.1G/16.1G [05:24<00:00, 49.5Mbyte/s]\n",
                        "INFO:hf-to-gguf:Model successfully exported to /content/model/unsloth.F16.gguf\n",
                        "Unsloth: Conversion completed! Output location: /content/model/unsloth.F16.gguf\n"
                    ]
                }
            ]
        },
        {
            "cell_type": "code",
            "source": [],
            "metadata": {
                "id": "EnT5DLHCxM0Y"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [],
            "metadata": {
                "id": "uflZubWEx5Pm"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}
